{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt install python-opengl\n",
    "    !apt install ffmpeg\n",
    "    !apt install xvfb\n",
    "    !pip install pyvirtualdisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    \n",
    "    # Start virtual display\n",
    "    dis = Display(visible=0, size=(600, 400))\n",
    "    dis.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. PPO\n",
    "\n",
    "- PPO: [J. Schulman et al., \"Proximal Policy Optimization Algorithms.\" arXiv preprint arXiv:1707.06347, 2017.](https://arxiv.org/abs/1707.06347.pdf)\n",
    "- TRPO: [Schulman, John, et al. \"Trust region policy optimization.\" International conference on machine learning. 2015.](http://proceedings.mlr.press/v37/schulman15.pdf)\n",
    "\n",
    "There are two kinds of algorithms of PPO: PPO-Penalty and PPO-Clip. Here, we'll implement PPO-clip version.\n",
    "\n",
    "TRPO computes the gradients with a complex second-order method. On the other hand, PPO tries to solve the problem with a first-order methods that keep new policies close to old. To simplify the surrogate objective, let $r(\\theta)$ denote the probability ratio\n",
    "\n",
    "$$ L^{CPI}(\\theta) = \\hat {\\mathbb{E}}_t \\left [ {\\pi_\\theta(a_t|s_t) \\over \\pi_{\\theta_{old}}(a_t|s_t)} \\hat A_t\\right] = \\hat {\\mathbb{E}}_t \\left [ r_t(\\theta) \\hat A_t \\right ].$$\n",
    "\n",
    "The objective is penalized further away from $r_t(\\theta)$\n",
    "\n",
    "$$ L^{CLIP}(\\theta)=\\hat {\\mathbb{E}}_t \\left [ \\min(r_t(\\theta) \\hat A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat A_t) \\right ] $$\n",
    "\n",
    "If the advantage is positive, the objective will increase. As a result, the action becomes more likely. If advantage is negative, the objective will decrease. AS a result, the action becomes less likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 777\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "We will use two separated networks for actor and critic respectively. The actor network consists of two fully connected hidden layer with ReLU branched out two fully connected output layers for mean and standard deviation of Gaussian distribution. Pendulum-v0 has only one action which has a range from -2 to 2. In order to fit the range, the actor outputs the mean value with tanh. The result will be scaled in ActionNormalizer class. On the one hand, the critic network has three fully connected layers as two hidden layers (ReLU) and an output layer. One thing to note is that we initialize the last layers' weights and biases as uniformly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3) -> nn.Linear:\n",
    "    \"\"\"Init uniform parameters on the single layer.\"\"\"\n",
    "    layer.weight.data.uniform_(-init_w, init_w)\n",
    "    layer.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    return layer\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int, \n",
    "        log_std_min: int = -20,\n",
    "        log_std_max: int = 0,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.hidden = nn.Linear(in_dim, 32)\n",
    "\n",
    "        self.mu_layer = nn.Linear(32, out_dim)\n",
    "        self.mu_layer = init_layer_uniform(self.mu_layer)\n",
    "\n",
    "        self.log_std_layer = nn.Linear(32, out_dim)\n",
    "        self.log_std_layer = init_layer_uniform(self.log_std_layer)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden(state))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.tanh(self.log_std_layer(x))\n",
    "        log_std = self.log_std_min + 0.5 * (\n",
    "            self.log_std_max - self.log_std_min\n",
    "        ) * (log_std + 1)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(in_dim, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        self.out = init_layer_uniform(self.out)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden(state))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE\n",
    "\n",
    "*GAE* help to reduce variance while maintaining a proper level of bias. By adjusting parameters $\\lambda \\in [0, 1]$ and $\\gamma \\in [0, 1]$. Please see [the paper](https://arxiv.org/pdf/1506.02438) for detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    next_value: list, \n",
    "    rewards: list, \n",
    "    masks: list, \n",
    "    values: list, \n",
    "    gamma: float, \n",
    "    tau: float\n",
    ") -> List:\n",
    "    \"\"\"Compute gae.\"\"\"\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns: Deque[float] = deque()\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = (\n",
    "            rewards[step]\n",
    "            + gamma * values[step + 1] * masks[step]\n",
    "            - values[step]\n",
    "        )\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.appendleft(gae + values[step])\n",
    "\n",
    "    return list(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent\n",
    "Here is a summary of PPOAgent class.\n",
    "\n",
    "| Method           | Note                                                 |\n",
    "|---               |---                                                   |\n",
    "|select_action     | select an action from the input state.               |\n",
    "|step              | take an action and return the response of the env.   |\n",
    "|update_model      | update the model by gradient descent.                |\n",
    "|train             | train the agent during num_frames.                   |\n",
    "|test              | test the agent (1 episode).                          |\n",
    "|_plot             | plot the training progresses.                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO updates the model several times(`epoch`) using the stacked memory. By `ppo_iter` function, It yield the samples of stacked memory by interacting a environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(\n",
    "    epoch: int,\n",
    "    mini_batch_size: int,\n",
    "    states: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    log_probs: torch.Tensor,\n",
    "    returns: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "):\n",
    "    \"\"\"Yield mini-batches.\"\"\"\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(epoch):\n",
    "        for _ in range(batch_size // mini_batch_size):\n",
    "            rand_ids = np.random.choice(batch_size, mini_batch_size)\n",
    "            yield states[rand_ids, :], actions[rand_ids], values[\n",
    "                rand_ids\n",
    "            ], log_probs[rand_ids], returns[rand_ids], advantages[rand_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"PPO Agent.\n",
    "    Attributes:\n",
    "        env (gym.Env): Gym env for training\n",
    "        gamma (float): discount factor\n",
    "        tau (float): lambda of generalized advantage estimation (GAE)\n",
    "        batch_size (int): batch size for sampling\n",
    "        epsilon (float): amount of clipping surrogate objective\n",
    "        epoch (int): the number of update\n",
    "        rollout_len (int): the number of rollout\n",
    "        entropy_weight (float): rate of weighting entropy into the loss function\n",
    "        actor (nn.Module): target actor model to select actions\n",
    "        critic (nn.Module): critic model to predict state values\n",
    "        transition (list): temporory storage for the recent transition\n",
    "        device (torch.device): cpu / gpu\n",
    "        total_step (int): total step numbers\n",
    "        is_test (bool): flag to show the current mode (train / test)        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        batch_size: int,\n",
    "        gamma: float,\n",
    "        tau: float,\n",
    "        epsilon: float,\n",
    "        epoch: int,\n",
    "        rollout_len: int,\n",
    "        entropy_weight: float,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epoch = epoch\n",
    "        self.rollout_len = rollout_len\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        # networks\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.critic = Critic(obs_dim).to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.005)\n",
    "\n",
    "        # memory for training\n",
    "        self.states: List[torch.Tensor] = []\n",
    "        self.actions: List[torch.Tensor] = []\n",
    "        self.rewards: List[torch.Tensor] = []\n",
    "        self.values: List[torch.Tensor] = []\n",
    "        self.masks: List[torch.Tensor] = []\n",
    "        self.log_probs: List[torch.Tensor] = []\n",
    "\n",
    "        # total steps count\n",
    "        self.total_step = 1\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # state = torch.FloatTensor(state).to(self.device)\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action, dist = self.actor(state)\n",
    "        selected_action = dist.mean if self.is_test else action\n",
    "\n",
    "        if not self.is_test:\n",
    "            value = self.critic(state)\n",
    "            self.states.append(state)\n",
    "            self.actions.append(selected_action)\n",
    "            self.values.append(value)\n",
    "            self.log_probs.append(dist.log_prob(selected_action))\n",
    "\n",
    "        return selected_action.cpu().detach().numpy()\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _, info = self.env.step(action)\n",
    "        next_state = np.reshape(next_state, (1, -1)).astype(np.float64)\n",
    "        reward = np.reshape(reward, (1, -1)).astype(np.float64)\n",
    "        done = np.reshape(done, (1, -1))\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.rewards.append(torch.FloatTensor(reward).to(self.device))\n",
    "            self.masks.append(torch.FloatTensor(1 - done).to(self.device))\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(\n",
    "        self, next_state: np.ndarray\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = self.critic(next_state)\n",
    "\n",
    "        returns = compute_gae(\n",
    "            next_value,\n",
    "            self.rewards,\n",
    "            self.masks,\n",
    "            self.values,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "        )\n",
    "\n",
    "        states = torch.cat(self.states).view(-1, 3)\n",
    "        actions = torch.cat(self.actions)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(self.values).detach()\n",
    "        log_probs = torch.cat(self.log_probs).detach()\n",
    "        advantages = returns - values\n",
    "\n",
    "        actor_losses, critic_losses = [], []\n",
    "\n",
    "        for state, action, old_value, old_log_prob, return_, adv in ppo_iter(\n",
    "            epoch=self.epoch,\n",
    "            mini_batch_size=self.batch_size,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            values=values,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            advantages=advantages,\n",
    "        ):\n",
    "            # calculate ratios\n",
    "            _, dist = self.actor(state)\n",
    "            log_prob = dist.log_prob(action)\n",
    "            ratio = (log_prob - old_log_prob).exp()\n",
    "\n",
    "            # actor_loss\n",
    "            surr_loss = ratio * adv\n",
    "            clipped_surr_loss = (\n",
    "                torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv\n",
    "            )\n",
    "\n",
    "            # entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            actor_loss = (\n",
    "                -torch.min(surr_loss, clipped_surr_loss).mean()\n",
    "                - entropy * self.entropy_weight\n",
    "            )\n",
    "\n",
    "            # critic_loss\n",
    "            value = self.critic(state)\n",
    "            #clipped_value = old_value + (value - old_value).clamp(-0.5, 0.5)\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "        \n",
    "            # train critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # train actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            actor_losses.append(actor_loss.item())\n",
    "            critic_losses.append(critic_loss.item())\n",
    "\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.values, self.masks, self.log_probs = [], [], []\n",
    "\n",
    "        actor_loss = sum(actor_losses) / len(actor_losses)\n",
    "        critic_loss = sum(critic_losses) / len(critic_losses)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "\n",
    "        state, info = self.env.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "\n",
    "        actor_losses, critic_losses = [], []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        while self.total_step <= num_frames + 1:\n",
    "            for _ in range(self.rollout_len):\n",
    "                self.total_step += 1\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done  = self.step(action)\n",
    "\n",
    "                state = next_state\n",
    "                score += reward[0][0]\n",
    "\n",
    "                # if episode ends\n",
    "                if done[0][0]:\n",
    "                    state, info = env.reset()\n",
    "                    state = np.expand_dims(state, axis=0)\n",
    "                    scores.append(score)\n",
    "                    score = 0\n",
    "\n",
    "                    self._plot(\n",
    "                        self.total_step, scores, actor_losses, critic_losses\n",
    "                    )\n",
    "\n",
    "            actor_loss, critic_loss = self.update_model(next_state)\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "        # termination\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _plot(\n",
    "        self,\n",
    "        frame_idx: int,\n",
    "        scores: List[float],\n",
    "        actor_losses: List[float],\n",
    "        critic_losses: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "\n",
    "        def subplot(loc: int, title: str, values: List[float]):\n",
    "            plt.subplot(loc)\n",
    "            plt.title(title)\n",
    "            plt.plot(values)\n",
    "\n",
    "        subplot_params = [\n",
    "            (131, f\"frame {frame_idx}. score: {np.mean(scores[-10:])}\", scores),\n",
    "            (132, \"actor_loss\", actor_losses),\n",
    "            (133, \"critic_loss\", critic_losses),\n",
    "        ]\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(30, 5))\n",
    "        for loc, title, values in subplot_params:\n",
    "            subplot(loc, title, values)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "*ActionNormalizer* is an action wrapper class to normalize the action values ranged in (-1. 1). Thanks to this class, we can make the agent simply select action values within the zero centered range (-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNormalizer(gym.ActionWrapper):\n",
    "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
    "\n",
    "    def action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = action * scale_factor + reloc_factor\n",
    "        action = np.clip(action, low, high)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = (action - reloc_factor) / scale_factor\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see [the code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py) and [configurations](https://github.com/openai/gym/blob/cedecb35e3428985fd4efad738befeb75b9077f1/gym/envs/__init__.py#L81) of Pendulum-v0 from OpenAI's repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunlong/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Pendulum-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "DeprecatedEnv",
     "evalue": "Environment version v0 for `Pendulum` is deprecated. Please use `Pendulum-v1` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71104/1598868914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Pendulum-v0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActionNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlatest_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlatest_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         raise error.DeprecatedEnv(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;34mf\"Environment version v{version} for `{get_env_id(ns, name, None)}` is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;34mf\"Please use `{latest_spec.id}` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m: Environment version v0 for `Pendulum` is deprecated. Please use `Pendulum-v1` instead."
     ]
    }
   ],
   "source": [
    "# environment\n",
    "env_id = \"Pendulum-v1\"\n",
    "env = gym.make(env_id)\n",
    "env = ActionNormalizer(env)\n",
    "\n",
    "\n",
    "# parameters\n",
    "num_frames = 100000\n",
    "\n",
    "agent = PPOAgent(\n",
    "    env,\n",
    "    gamma = 0.9,\n",
    "    tau = 0.8,\n",
    "    batch_size = 64,\n",
    "    epsilon = 0.2,\n",
    "    epoch = 64,\n",
    "    rollout_len = 2048,\n",
    "    entropy_weight = 0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "if IN_COLAB:\n",
    "    agent.env = gym.wrappers.Monitor(agent.env, \"videos\", force=True)\n",
    "frames = agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:  # for colab\n",
    "    import base64\n",
    "    import glob\n",
    "    import io\n",
    "    import os\n",
    "\n",
    "    from IPython.display import HTML, display\n",
    "\n",
    "    def ipython_show_video(path: str) -> None:\n",
    "        \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
    "        if not os.path.isfile(path):\n",
    "            raise NameError(\"Cannot access: {}\".format(path))\n",
    "\n",
    "        video = io.open(path, \"r+b\").read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        display(HTML(\n",
    "            data=\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
    "            </video>\n",
    "            \"\"\".format(encoded.decode(\"ascii\"))\n",
    "        ))\n",
    "\n",
    "    list_of_files = glob.glob(\"videos/*.mp4\")\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    ipython_show_video(latest_file)\n",
    "\n",
    "else:  # for jupyter\n",
    "    from matplotlib import animation\n",
    "    from JSAnimation.IPython_display import display_animation\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    def display_frames_as_gif(frames):\n",
    "        \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
    "        patch = plt.imshow(frames[0])\n",
    "        plt.axis('off')\n",
    "\n",
    "        def animate(i):\n",
    "            patch.set_data(frames[i])\n",
    "\n",
    "        anim = animation.FuncAnimation(\n",
    "            plt.gcf(), animate, frames = len(frames), interval=50\n",
    "        )\n",
    "        display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "\n",
    "    # display \n",
    "    display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "82cde04e37cbe455b040118feccad411939a9271591370c307cdd833bd7555ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
