{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Inverted Pendulum Using Reinforcement Learning \n",
    "\n",
    "![SegmentLocal](pendulum.gif \"segment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Introduction\n",
    "\n",
    "In this task, we are going to train a neural network policy for inverted pendulum using reinforcement learning. \n",
    "The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point. The diagram below specifies the coordinate system used for the implementation of the pendulum's dynamic equations.\n",
    "\n",
    "\n",
    "### Action Space\n",
    "The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
    "\n",
    "| Num | Action | Min  | Max |\n",
    "|-----|--------|------|-----|\n",
    "| 0   | Torque | -2.0 | 2.0 |\n",
    "\n",
    "### Observation Space\n",
    "The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free\n",
    "end and its angular velocity.\n",
    "\n",
    "| Num | Observation      | Min  | Max |\n",
    "|-----|------------------|------|-----|\n",
    "| 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
    "| 1   | y = sin(theta)   | -1.0 | 1.0 |\n",
    "| 2   | Angular Velocity | -8.0 | 8.0 |\n",
    "\n",
    "### Rewards\n",
    "\n",
    "The reward function is defined as:\n",
    "$$r = -(theta^2 + 0.1 * \\hat{theta}^2 + 0.001 * torque^2)$$\n",
    "\n",
    "where $\\theta$ is the pendulum's angle normalized between $[-pi, pi]$ (with 0 being in the upright position).\n",
    "\n",
    "(You don't need to define the reward. It is provided by the OpenAI Gym environment)\n",
    "\n",
    "### Starting State\n",
    "\n",
    "The starting state is a random angle in *[-pi, pi]* and a random angular velocity in *[-1,1]*.\n",
    "\n",
    "### Episode Truncation\n",
    "The episode truncates at 200 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementation\n",
    "\n",
    "\n",
    "### Import python packages \n",
    "\n",
    "\n",
    "We will use **OpenAI Gym** as our simulator. Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # gym.__version__ = 0.20.0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We provide some default hyperparameters. Note that RL is very sensitive to hyperparameters. Feel free to modify the them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate  = 0.0003\n",
    "gamma           = 0.9\n",
    "lmbda           = 0.9\n",
    "eps_clip        = 0.2\n",
    "K_epoch         = 10\n",
    "rollout_len    = 3\n",
    "buffer_size    = 30\n",
    "minibatch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Your trask is to implement the RL loss function for updating the neural network policy. We use the so called **Advantage Function** to estimate the policy gradient. \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} J(\\theta) &= \\mathbb{E} \\large[ \\sum_{k=0}^{N} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_k | s_k) A(s_k, a_k) \\large] \\\\\n",
    "\\theta_{new} &= \\theta_{old} + \\alpha \\nabla_{\\theta} J(\\theta) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $A(s_k, a_k)$ is an advantage function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RLAgent, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        \n",
    "        self.fc1   = nn.Linear(3,128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.fc_v = nn.Linear(128,1)\n",
    "        \n",
    "        # Define network optimizer for updating the parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        # # Policy network \n",
    "        # observation dimension = 3\n",
    "        # action dimension = 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # the mean action\n",
    "        mu = 2.0*torch.tanh(self.fc_mu(x))\n",
    "        \n",
    "        # the action exploration (state-dependent exploration)\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        return mu, std\n",
    "    \n",
    "    def v(self, x):\n",
    "        # # Value function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "                    \n",
    "                    s_lst.append(s)\n",
    "                    a_lst.append([a])\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append([prob_a])\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_lst.append([done_mask])\n",
    "\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "                    \n",
    "            mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "\n",
    "        return data_with_adv\n",
    "\n",
    "        \n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            \n",
    "            # compute advantage function\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "                    \n",
    "                    # ====================== This is your task ============================\n",
    "                    #\n",
    "                    # Please implement the policy update here using policy gradient. \n",
    "                    # \n",
    "                    # TODOs: \n",
    "                    # Step 1: compute loss \n",
    "                    # Step 2: update network parameters via backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "model = RLAgent()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "rollout = []\n",
    "\n",
    "for n_epi in range(10000):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        for t in range(rollout_len):\n",
    "            # compute mean and std\n",
    "            mu, std = model.pi(torch.from_numpy(s).float())\n",
    "            dist = Normal(mu, std)\n",
    "            \n",
    "            # sample control command\n",
    "            a = dist.sample()\n",
    "            log_prob = dist.log_prob(a)\n",
    "            \n",
    "            # simulate the environment. \n",
    "            s_prime, r, done, info = env.step([a.item()])\n",
    "\n",
    "            # collect data \n",
    "            rollout.append((s, a, r/10.0, s_prime, log_prob.item(), done))\n",
    "            if len(rollout) == rollout_len:\n",
    "                model.put_data(rollout)\n",
    "                rollout = []\n",
    "\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Network training. \n",
    "        # ====================== This is your task ============================\n",
    "        model.train_net()\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}, opt step: {}\".format(n_epi, score/print_interval, model.optimization_step))\n",
    "        score = 0.0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
