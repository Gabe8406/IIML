{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c512fc59",
   "metadata": {},
   "source": [
    "# Programming exercise week 12\n",
    "\n",
    "\n",
    "## Time series forecasting with LSTMs\n",
    "\n",
    "In this exercise we will demonstrate how to forecast time series data using Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN). For more information on LSTMs you can refer to the following materials...\n",
    "\n",
    "Sequence Models and Long Short-Term Memory Networks\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "LSTM implementation in PyTorch (documentation)\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "Your exercises are marked in code with a comment `FINISH THE CODE` - please fill them in! There are 5 exercises in section 1 (Forecasting synthetic time series data) and 4 exercises in section 2 (Forecasting Bitcoin price data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf39c92",
   "metadata": {},
   "source": [
    "## 1. Forecasting synthetic time series data\n",
    "\n",
    "First part of the exercise will involve forecasting of the synthetic time series data consisting of *trend*, *periodic* and *noise components*. You can change the parameters to make your own time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cdf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple artificial time series consisting of trend, periodic and noise component\n",
    "num_samples = 200\n",
    "num_periods = 3\n",
    "sample_span = [0,2*num_periods*np.pi]\n",
    "trend_strength = 0.1\n",
    "periodic_strength = 1.0\n",
    "noise_strength = 0.2\n",
    "domain = np.linspace(sample_span[0],sample_span[1],num_samples)\n",
    "signal = trend_strength*domain+\\\n",
    "         periodic_strength*np.sin(domain)+\\\n",
    "         noise_strength*np.random.randn(num_samples)                       \n",
    "\n",
    "\n",
    "plt.plot(domain,signal)\n",
    "plt.ylabel('signal')\n",
    "plt.xlabel('domain')\n",
    "plt.title('Synthetic time series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875e7f1",
   "metadata": {},
   "source": [
    "The function `create_sequences()` creates the training subsequences of length `train_window_size` and a target sample which is the next in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be68df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the training sequences\n",
    "def create_sequences(input_data, tw):\n",
    "    seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        # Allow both multiple and single time series\n",
    "        # In case of multiple time series, we assume that the labels are drawn from the first time series!\n",
    "        if len(input_data.size())==1:\n",
    "            train_sequence = input_data[i:i+tw]\n",
    "            train_label = input_data[i+tw:i+tw+1]\n",
    "        else:\n",
    "            train_sequence = input_data[i:i+tw,:]\n",
    "            train_label = input_data[i+tw:i+tw+1,1]\n",
    "        seq.append((train_sequence ,train_label))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last test_size samples from the training set\n",
    "test_size = 50\n",
    "\n",
    "# 1.2. FINISH THE CODE - define train and test datasets!\n",
    "train = # FINISH THE CODE\n",
    "test = # FINISH THE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccb7f0",
   "metadata": {},
   "source": [
    "We will be scaling our data to the $[-1,1]$ range using the [`sklearn.preprocessing.MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to the range -1 to 1\n",
    "\n",
    "# 1.3. FINISH THE CODE - define a MinMaxScaler with feature range -1 to 1\n",
    "scaler = # FINISH THE CODE\n",
    "\n",
    "train_norm = scaler.fit_transform(train.reshape(-1, 1))\n",
    "train_norm = torch.FloatTensor(train_norm).view(-1)\n",
    "train_norm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_size = 14\n",
    "train_sequence = create_sequences(train_norm, train_window_size)\n",
    "train_sequence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df891e64",
   "metadata": {},
   "source": [
    "Here we will create our LSTM model class which will inherit from the [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class from the PyTorch library. The initial configuration for our model will take following inputs:\n",
    "\n",
    "- `input_size` - Number of features in input. Our sequence is of length `train_window_size` but the actual input to the model is equal to 1 because we only have a single time series.\n",
    "- `hidden_layer_size` - the number of hidden layers along with the number of neurons in each layer.\n",
    "- `output_size` - Number of outputs. We will set it to 1 as we are only predicting one additional sample in the future. Predicting farther into the future will be performed by iterativelly calling our model on already predicted sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdbf8f",
   "metadata": {},
   "source": [
    "As we are doing a regression we will use mean squared error loss implemented in [`nn.MSELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65befc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "input_size = 1 # number of time series features in input\n",
    "hidden_layer_size = 100 # size of the hidden layers, we only have one hidden layer with 100 neurons\n",
    "output_size = 1 # number of outputs, we only predict one day in the future\n",
    "\n",
    "model = LSTM(input_size,hidden_layer_size,output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745088ed",
   "metadata": {},
   "source": [
    "We will do the model training for a specified number of epochs, storing the intermediary losses for later plotting. We expect that the loss will decrease on average over time, which means that our model is successfully learning from input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the specified number of epochs\n",
    "epochs = 100\n",
    "\n",
    "loss_array = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Make sure that model is in the train mode\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_sequence:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        # 1.4. FINISH THE CODE - apply your model to the current sequence and calculate the loss\n",
    "        y_pred = # FINISH THE CODE\n",
    "        loss = # FINISH THE CODE\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (i+1)%5 == 0:\n",
    "        seconds = (time.time()-start_time)\n",
    "        print(f'\\repoch: {(i+1):3}/{epochs:3} loss: {loss.item():10.8f}, time elapsed: {seconds:.0f} s',end='')\n",
    "        loss_array.append([i,loss.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98244601",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in loss_array],[x[1] for x in loss_array])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Learning curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96462470",
   "metadata": {},
   "source": [
    "After the initial training we will run the training for several more epochs in order to get multiple forecasting models. By plotting all of their predictions together we will get an estimate of the variance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run several more epochs and output their predictions\n",
    "predictions_dict = {}\n",
    "output_epochs = 30\n",
    "start_time = time.time()\n",
    "    \n",
    "for i in range(output_epochs):\n",
    "        \n",
    "    # Step 1: Train the model for one more epoch\n",
    "\n",
    "    # Make sure that model is in the train mode\n",
    "    model.train()\n",
    "    for seq, labels in train_sequence:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        # 1.5. FINISH THE CODE - apply your model to the current sequence and calculate the loss\n",
    "        # NOTE - this part is identical to the one you already solved in previous exercise!\n",
    "        y_pred = # FINISH THE CODE\n",
    "        loss = # FINISH THE CODE\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step 2: Test the model\n",
    "    test_inputs = train_norm[-train_window_size:].tolist()\n",
    "\n",
    "    # Set the model in evaluation (inference) mode\n",
    "    model.eval()\n",
    "\n",
    "    # Apply the model iterativelly to generate predictions in the future\n",
    "    for j in range(test_size):\n",
    "        seq = torch.FloatTensor(test_inputs[-train_window_size:])\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            \n",
    "    predictions_dict[i] = scaler.inverse_transform(np.array(test_inputs[train_window_size:] ).reshape(-1, 1))\n",
    "    \n",
    "    if (i+1)%5 == 0:\n",
    "        seconds = (time.time()-start_time)\n",
    "        print(f'\\repoch: {(i+1):3}/{output_epochs:3}, time elapsed: {seconds:.0f} s',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9ec4f",
   "metadata": {},
   "source": [
    "Lets plot all of the predictions from our model on a single figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multiple predictions\n",
    "plt.title('Forecasting of synthetic time series')\n",
    "plt.ylabel('signal')\n",
    "plt.xlabel('domain')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(domain,signal)\n",
    "\n",
    "for prediction in predictions_dict.values():\n",
    "    plt.plot(domain[-test_size:],prediction,c='b',alpha=0.2)\n",
    "\n",
    "plt.axvline(x=domain[-test_size], c='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multiple predictions on a restricted time scale\n",
    "plt.title('Forecasting of synthetic time series')\n",
    "plt.ylabel('signal')\n",
    "plt.xlabel('domain')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "\n",
    "plt.plot(domain[-test_size:],signal[-test_size:])\n",
    "\n",
    "for prediction in predictions_dict.values():\n",
    "    plt.plot(domain[-test_size:],prediction,c='b',alpha=0.2)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ca12b",
   "metadata": {},
   "source": [
    "Finally, we will plot the same mutliple predictions as before but we will superimpose the kernel density function on top of it to better visualize predictions which appear in multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions as a density estimation\n",
    "\n",
    "# Construct the point arrays on which density will be estimated\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "d = np.array([])\n",
    "for prediction in predictions_dict.values():\n",
    "    d = np.append(d,domain[-test_size:])\n",
    "    x = np.append(x,np.arange(test_size))\n",
    "    y = np.append(y,prediction)\n",
    "\n",
    "# Calculate the density through Gaussian kernel density estimation\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density, so that the densest points are plotted last\n",
    "idx = z.argsort()\n",
    "x, y, z, d = x[idx], y[idx], z[idx], d[idx]\n",
    "\n",
    "# Plot the original time series\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Forecasting of synthetic time series')\n",
    "plt.ylabel('signal')\n",
    "plt.xlabel('domain')\n",
    "ax.grid(True)\n",
    "ax.autoscale(axis='x', tight=True)\n",
    "\n",
    "# Plot the density estimations\n",
    "ax.plot(domain[-test_size:],signal[-test_size:])\n",
    "ax.scatter(d, y, c=z, s=50)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a12c2",
   "metadata": {},
   "source": [
    "## 2. Forecasting Bitcoin price data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eda51d",
   "metadata": {},
   "source": [
    "In this exercise we will be doing a time series forecasting of Bitcoin price. The CSV dataset of Bitcoin price is downloaded from [Coincodex](https://coincodex.com/crypto/bitcoin/historical-data/) - make sure that you have `bitcoin_2017-11-10_2022-11-09.csv` in the same folder as this notebook. For simplicity we will restrict the training of the model on the last `sample_size` samples, but you can run it on the whole time series.\n",
    "\n",
    "The training and testing procedure is identical as in the example with the synthetic time series, and we will be using the same `create_sequences()` function and `LSTM` model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bce001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time series samples we will use for analysis\n",
    "sample_size = 200\n",
    "\n",
    "# Load Bitcoin price data from CSV file \n",
    "# NOTE the data is within the repository under the Lecture 12 Economics and Finance + ML in blockchain applications folder\n",
    "price = pd.read_csv(\"YOUR PATH\", index_col = 'Date', parse_dates=True)\n",
    "price = price.iloc[::-1] # Reverse rows so that earlier dates come first\n",
    "price = price.iloc[-sample_size:] # We only take sample_size most recent data points\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Bitcoin Close price time series\n",
    "ax = price['Close'].plot(ylabel='USD', title='Bitcoin close price')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bitcoin Close data on which we will do forecasting\n",
    "close = price[['Close']].to_numpy().flatten()\n",
    "close[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc69ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last test_size samples from the training set\n",
    "test_size = 50\n",
    "\n",
    "# 2.1. FINISH THE CODE - define train and test datasets!\n",
    "train = # FINISH THE CODE\n",
    "test = # FINISH THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to the range -1 to 1\n",
    "\n",
    "# 2.2. FINISH THE CODE - define a MinMaxScaler with feature range -1 to 1\n",
    "scaler = # FINISH THE CODE\n",
    "\n",
    "train_norm = scaler.fit_transform(train.reshape(-1, 1))\n",
    "train_norm = torch.FloatTensor(train_norm).view(-1)\n",
    "train_norm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d571476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_size = 14\n",
    "train_sequence = create_sequences(train_norm, train_window_size)\n",
    "train_sequence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "input_size = 1 # number of time series features in input\n",
    "hidden_layer_size = 100 # size of the hidden layers, we only have one hidden layer with 100 neurons\n",
    "output_size = 1 # number of outputs, we only predict one day in the future\n",
    "\n",
    "model = LSTM(input_size,hidden_layer_size,output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "epochs = 100\n",
    "\n",
    "loss_array = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Make sure that model is in the train mode\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_sequence:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        \n",
    "        # 2.3. FINISH THE CODE - apply your model to the current sequence and calculate the loss\n",
    "        y_pred = # FINISH THE CODE\n",
    "        loss = # FINISH THE CODE\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (i+1)%5 == 0:\n",
    "        seconds = (time.time()-start_time)\n",
    "        print(f'\\repoch: {(i+1):3}/{epochs:3} loss: {loss.item():10.8f}, time elapsed: {seconds:.0f} s',end='')\n",
    "        loss_array.append([i,loss.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in loss_array],[x[1] for x in loss_array])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Learning curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run several more epochs and output their predictions\n",
    "predictions_dict = {}\n",
    "output_epochs = 30\n",
    "start_time = time.time()\n",
    "    \n",
    "for i in range(output_epochs):\n",
    "        \n",
    "    # Step 1: Train the model for one more epoch\n",
    "\n",
    "    # Make sure that model is in the train mode\n",
    "    model.train()\n",
    "    for seq, labels in train_sequence:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        # 2.4. FINISH THE CODE - apply your model to the current sequence and calculate the loss\n",
    "        # NOTE - this part is identical to the one you already solved in previous exercise!\n",
    "        y_pred = # FINISH THE CODE\n",
    "        loss = # FINISH THE CODE)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step 2: Test the model\n",
    "    test_inputs = train_norm[-train_window_size:].tolist()\n",
    "\n",
    "    # Set the model in evaluation (inference) mode\n",
    "    model.eval()\n",
    "\n",
    "    # Apply the model iterativelly to generate predictions in the future\n",
    "    for j in range(test_size):\n",
    "        seq = torch.FloatTensor(test_inputs[-train_window_size:])\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            \n",
    "    predictions_dict[i] = scaler.inverse_transform(np.array(test_inputs[train_window_size:] ).reshape(-1, 1))\n",
    "    \n",
    "    if (i+1)%5 == 0:\n",
    "        # print(f'epoch: {i:3}')\n",
    "        seconds = (time.time()-start_time)\n",
    "        print(f'\\repoch: {(i+1):3}/{output_epochs:3}, time elapsed: {seconds:.0f} s',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae232dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multiple predictions\n",
    "plt.title('Prediction of Bitcoin close daily price')\n",
    "plt.ylabel('USD')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(price['Close'])\n",
    "\n",
    "for prediction in predictions_dict.values():\n",
    "    plt.plot(price.index[-test_size:],prediction,c='b',alpha=0.2)\n",
    "\n",
    "plt.axvline(x=price.index[-test_size], c='r', linestyle='--')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multiple predictions on a restricted time scale\n",
    "plt.title('Prediction of Bitcoin close daily price')\n",
    "plt.ylabel('USD')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "\n",
    "plt.plot(price['Close'][-test_size:])\n",
    "\n",
    "for prediction in predictions_dict.values():\n",
    "    plt.plot(price.index[-test_size:],prediction,c='b',alpha=0.2)\n",
    "    \n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459355ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions as a density estimation\n",
    "\n",
    "# Construct the point arrays on which density will be estimated\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "d = np.array([],dtype=np.datetime64)\n",
    "for prediction in predictions_dict.values():\n",
    "    d = np.append(d,price.index[-test_size:].values)\n",
    "    x = np.append(x,np.arange(test_size))\n",
    "    y = np.append(y,prediction)\n",
    "\n",
    "# Calculate the density through Gaussian kernel density estimation\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density, so that the densest points are plotted last\n",
    "idx = z.argsort()\n",
    "x, y, z, d = x[idx], y[idx], z[idx], d[idx]\n",
    "\n",
    "# Plot the original time series\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Prediction of Bitcoin close daily price')\n",
    "ax.set_ylabel('USD')\n",
    "ax.grid(True)\n",
    "ax.autoscale(axis='x', tight=True)\n",
    "\n",
    "# Plot the density estimations\n",
    "ax.plot(price['Close'][-test_size:])\n",
    "ax.scatter(d, y, c=z, s=50)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
