{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b032acf9-09da-43aa-81d7-70f40f161c09",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "In this first task you will implement a toy version of the Word2Vec algorithm to produce some simple word embeddings using the Pytorch library. <br>\n",
    "First, we need to install the reqired packages (if you have not installed them already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38562e32-0221-4178-ac94-e07c19ae1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01332f2b-fddc-404e-8963-28002bea4fad",
   "metadata": {},
   "source": [
    "<br>We will need a corpus to train our embeddings on. To keep computation times low and to be able to produce simpler plots later, we will only use the short text below as a toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d268f-772d-41e7-8990-0b674acf59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Human language is special for several reasons . It is specifically constructed to convey the speaker / writer's \" \\\n",
    "       \"meaning . It is a complex system , although little children can learn it pretty quickly . Another remarkable \" \\\n",
    "       \"thing about human language is that it is all about symbols . According to Chris Manning , a machine learning \" \\\n",
    "       \"professor at Stanford , it is a discrete , symbolic , categorical signaling system . This means we can convey the \" \\\n",
    "       \"same meaning in different ways ( i.e. , speech , gesture , signs , etc. ) The encoding by the human brain is a \" \\\n",
    "       \"continuous pattern of activation by which the symbols are transmitted via continuous signals of sound and \" \\\n",
    "       \"vision . Understanding human language is considered a difficult task due to its complexity . For example , there \" \\\n",
    "       \"are an infinite number of different ways to arrange words in a sentence . Also , words can have several meanings \" \\\n",
    "       \"and contextual information is necessary to correctly interpret sentences . Every language is more or less unique \" \\\n",
    "       \"and ambiguous . Just take a look at the following newspaper headline \\\" The Popeâ€™s baby steps on gays . \\\" This \" \\\n",
    "       \"sentence clearly has two very different interpretations , which is a pretty good example of the challenges in \" \\\n",
    "       \"NLP . Note that a perfect understanding of language by a computer would result in an AI that can process the \" \\\n",
    "       \"whole information that is available on the internet , which in turn would probably result in artificial general \" \\\n",
    "       \"intelligence .\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02aa63b-285f-4131-a37d-85f36a4a61e2",
   "metadata": {},
   "source": [
    "<br>In order to train our model, we first need to transform this text into usable training data.\n",
    "1. Extract the vocabulary from this text. Do not worry about collapsing different forms of the same word into a single entry (e.g. treat \"language\" and \"languages\" as separate words). You also do not need to worry about punctuation marks. They are already split from the words in the text where necessary, so you can treat them like individual words. This should result in a list with a single entry for each unique word (make sure to not have multiple entries for e.g. \"is\").\n",
    "2. Assign an index to each token in the vocabulary, so that each word can be represented as a distinct number.\n",
    "3. Transform our corpus into a series of these indeces.\n",
    "4. One-Hot encode the corpus using the <i>torch.nn.functional.one_hot()</i> function. You can find the documentation for it [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html).\n",
    "5. Convert the resulting tensor so that it contains floating point numbers instead of integers (this is just to make it compatible with our model implementation later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf86735-64cd-458a-b2be-a5d7bd174c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc732b2-b326-49c2-a48a-d3772b736d68",
   "metadata": {},
   "source": [
    "<br>Now costruct the actual training data. We are going to implement the skip-gram version of Word2Vec, using a context window of size 5. This means that for each word, the model should try to predict the two words that come before and after it individually. Thus, the training data should consist of four input-label pairs for each word, for which the four labels are the words in the context window. E.g. for the word \"special\" in the first sentence of our toy corpus, the pairs would be (special, language), (special, is), (special, for), (special, several). Keep in mind, however, that for our model implementation, we will need two tensors, and not a list of tuples. So you should construct an input tensor containing the one-hot encoded tokens from our toy-corpus four times, and for each of these four entries the label tensor should contain a different word in its context window. Since we are going to use CrossEntropyLoss as our loss function later, the labels should not be one-hot encoded, but simply state the index of the target token (this is simply a characteristic of the torch implementation of cross entropy loss). You might have noticed that this way of constructing training data is problematic for the first two and the last two tokens in the corpus, because their context window would be out of bounds. Usually, this would be solved by padding (adding two special tokens at the beginning and at the end of the text), but for simplicity's sake, you can just ignore the first and last two tokens in the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b9972-e684-4741-b3b4-3a0e34881c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d7999-4dc1-45ec-9cf3-1c9e2c0ddb63",
   "metadata": {},
   "source": [
    "The next step is to construct our model. For easier visualisation later, we want to produce a model with embedding size 2. In reality, Word2Vec models use a much bigger embedding size, usually around 300.<br>\n",
    "Pytorch uses classes to define models. You can find a code skeleton for such a class below. Define a model in such a way, that it has three layers, an input, a hidden and an output layer. Also apply the Softmax activation function to the output.<br>\n",
    "We use two fully connected linear networks to connect the input layer to the hidden layer, and the latter to the output layer. The input and output size should be identical and be equal to the size of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd376dd-3f39-4d9b-8d84-33a0d5287201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = # Linear network connecting input to hidden layer\n",
    "        self.prediction = # Linear network connecting embedding to output layer\n",
    "        self.activation = torch.nn.Softmax(dim=0)\n",
    "    def forward(self, x):\n",
    "        # apply the layers defined above to the input x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6dd8d3-de58-4962-a16f-0256ad95016e",
   "metadata": {},
   "source": [
    "Now you should instantiate our model. Additionally, define a learning rate (how much the weights of our model should change after each training iteration, usually around 0.01) and how many epochs (how many iterations over the training data) you want to do. More epochs can increase model performance, but also increases computation time, and, because our training data is so small, the model should converge rather quickly, so do not choose an overly high number of epochs. A good point to start might be to choose 100 epochs and then adjust that number, depending on how long your code takes to run. As stated above, we are going to use cross entropy loss as our loss function and Adam as our optimizer (already implemented in the code below). An optimizer handles the way in which the weights of our model should be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c94c57-3204-40d5-8012-9e79c8435583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec()\n",
    "lr = # learning rate\n",
    "epochs = # number of epochs\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958f948-9435-4fc7-9ce0-36f26a672f8c",
   "metadata": {},
   "source": [
    "It is time to train our model now.\n",
    "1. For each epoch, loop through all training instances.\n",
    "2. We update our weights after each instance. So you should call your model on each sample, to get a prediction (model(sample)).\n",
    "3. Compute the loss of each prediction each prediction, by using our defined loss function (Cross Entropy Loss). Pass the prediction and the vocabulary index of the correct word to the loss function.\n",
    "4. Then, use the <i>.backward()</i> on your loss, and use <i>optimizer.step()</i> and <i>optimizer.zero_grad()</i>. This updates the weights through backpropagation and resets the optimizer gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76868594-7d4a-49b0-9b46-b8891070e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a81d6-5030-4ee9-87d0-263ac3ec3dca",
   "metadata": {},
   "source": [
    "Modify the following code to visualise your embeddings. You simply need to save the hidden layer weights into the <i>weights</i> variable. In the original implementation of Word2Vec, the prediction weights were used as embeddings and not the hidden weights (Mikolov et al., 2013, [Link](https://arxiv.org/abs/1301.3781)). However, in practice, both versions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3368f2-690e-429a-adc7-5811571f53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "weights = # This should look something like my_model.hidden.weight\n",
    "weights.detach_()\n",
    "weights_dim_1 = weights[0]\n",
    "weights_dim_2 = weights[1]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "plt.scatter(weights_dim_1, weights_dim_2, s=14)\n",
    "for i, word in enumerate(uniques):\n",
    "    plt.annotate(word, (weights_dim_1[i], weights_dim_2[i]), size= 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73bc22-ae51-4b40-abf3-cad93f646a12",
   "metadata": {},
   "source": [
    "Obviously, our training data is way too small and our model too simple to produce good embeddings for all tokens in the corpus.<br>\n",
    "Can you still find some tokens that our model produced meaningful embeddings for? (Hint: look for clusters of tokens you would expect to be close to each other in a vector space representation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6e033-55eb-4293-b7d2-53cbd3d4a103",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb78ec-8d81-4b9f-bd89-67c9930bcadd",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "In this task, you will finetune a version of BERT (Bidirectional Encoder Representations from Transformers), a transformer based language model, to classify movie reviews on Rotten Tomatoes into positive or negative reviews. The goal is to train the model to predict whether a review is positive (1) or negative (0).\n",
    "As in task one, first install the following packages, if you have not done so already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c320e-513d-4075-a313-ae7e96595f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install datasets\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723430ef-6de5-4bc3-af92-d0f0672638b0",
   "metadata": {},
   "source": [
    "<br>Download the dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febc79d-066e-42e1-86e0-708710e9d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('rotten_tomatoes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0c4cf-64a4-4fa1-9fc4-b037fb9f5fc1",
   "metadata": {},
   "source": [
    "Look at some entries in the dataset, to familiarize yourself with its contents and structure. You can also have a look at [this website](https://huggingface.co/datasets/rotten_tomatoes) to get some additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838efa1-db0c-47a4-b4ae-4dd877d73a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for data exploration ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237585c-da6f-44c9-968c-9ffb3182716e",
   "metadata": {},
   "source": [
    "To be able to use the texts in the database, we need them in tokenized form. For this you need to download a tokenizer. The following code snippet downloads a pretrained tokenizer, and tokenizes the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf5e7f-a3fe-4334-8578-741a300a98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b154c7-de98-43d3-9be4-360ad7a53043",
   "metadata": {},
   "source": [
    "Using the whole data set would probably make the training process very long, therefore you should only use a small subset of the training and test instances contained in the data set. (Hint: use the .shuffle and .select method of the Dataset class. The documentation can be found [here](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822a3b3-54ad-4dc8-a652-c7c68d94a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f23c6-88d7-4486-939e-79aa186b5e58",
   "metadata": {},
   "source": [
    "Load the distilbert model (a smaller version of the full BERT model) and the evaluation metric to be used (in this case 'accuracy'). Fill in the number of labels the classification model should use. (Do not worry about the warning, it does not concern the task at hand.) The model will take a series of input tokens, which is why we had to tokenise the reviews, and returns a logit for each possible label, which indicates how likely it is that the input sequence corresponds to that label, according to the model's prediction. The evaluation metric is used to calculate how likely the predictions that the model makes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3450a6-94e0-499c-b5f9-3784776a1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=) # fill in num_labels\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd50b57-b1ed-49ac-95db-44d86275a79c",
   "metadata": {},
   "source": [
    "In order to compute the metric for the predictions our model makes, we will need to first convert the logits that the model returns into the corresponding class label. The following function will do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2afa43-b374-45ad-9ea0-bbcf4c27435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb14e0d-c737-4c13-9a04-706493376b14",
   "metadata": {},
   "source": [
    "Before training, specify the training arguments. The most important ones here are the number of epochs to be used and the evaluation strategy: Use 'epoch' as an argument to evaluate your model after each epoch or 'step' to evaluate it after each weight update step. As for the number of epochs, since this model does many more computations per epoch compared to the one you implemented in task 1, you should choose a much lower value. A good starting point is to set the num_training_epochs parameter to 3, and then adjust as needed. If you are interested and would like to tweak other arguments, you can find a list of them [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737d95a-e623-4d00-83ca-da49ee760eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir='my_training_dir', evaluation_strategy=\"epoch\", num_train_epochs=) # tweak training args to your liking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb49864-fb7e-419a-a213-206d23eac72a",
   "metadata": {},
   "source": [
    "The transformers library comes with a Trainer class, that streamlines the training process. Create an instance of this class and pass your base model, your training arguments, your training data set, your evaluation data set and the compute_metrics function to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e492c6d-2488-4e13-92e9-7ac99931bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=,\n",
    "    eval_dataset=,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea22cdc-ea63-42c4-baec-bba5c7ff399f",
   "metadata": {},
   "source": [
    "Start the training by calling .train() on your trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625b479-23cf-4df2-b376-7facb0dae8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3b166-1251-4340-ae4b-7e2ae9dc2cc6",
   "metadata": {},
   "source": [
    "What accuracy did your model achieve at the end of the training process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f505ed-800b-4764-a66d-e25ff92b8067",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9614d52-6f8b-4cd1-a60c-6dcf2b257194",
   "metadata": {},
   "source": [
    "Now you should do some predictions using your model. Use the TextClassificationPipeline class included in the transformers library to write a function which takes any string as an input and returns 'POSITIVE' if your model thinks the input was a positive review, and 'NEGATIVE' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93534c8-2812-4c43-8c8d-cb521dc6e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "def predict(review:str) -> str:\n",
    "    ### Your code here ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a9d22-141a-42e7-906a-0bbfe286339e",
   "metadata": {},
   "source": [
    "Could you find any kind of prompts your model struggles to classify correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da401832-d0fa-49e7-b0d3-39985d0bc99e",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e639e5-4617-4452-b4bc-4d9d0911a9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
