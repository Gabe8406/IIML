{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8061a72a",
   "metadata": {},
   "source": [
    "# Non-Programming Exercise Week 4 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9feada-59ca-498a-8ae8-9f8743645abd",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "The tf-idf (term frequency - inverse document frequency) is a measure of how important a specific word in a document $doc$ is to the document itself, within the context of a collection of documents $D$. \n",
    "\n",
    "It has been used in a variety of NLP-applications. In this task, you will calculate some tf-idf values for words in short texts by hand, in order to gain some intuition on how it works. \n",
    "\n",
    "The term frequency is calculated using the following formula: \n",
    "\n",
    "\n",
    "${\\sf tf}(w) = \\frac{C_w}{C_t}$\n",
    "\n",
    "where w is the word in question, $C_w$ is the total count of the word in the document and $C_t$ is the total word count of the document.\n",
    "\n",
    "Likewise, the inverse document frequency is calculated by this formula:\n",
    "\n",
    "${\\sf idf}(w)=\\log(\\frac{N}{|\\{doc \\in D:w \\in doc\\}|})$\n",
    "\n",
    "where $N$ is the total number of documents. The denominator here denotes the number of documents in the collection that contain the word $w$, and the whole fraction is then logarithmically scaled using the natural logarithm.\n",
    "\n",
    "To calculate the final tf-idf value, the two values simply need to be multiplied. This causes the tf to be scaled by the idf, so that words that appear in many or all documents are weighted less than words that appear in few documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938541b5-39ba-4cf4-ab5d-8f5561ba9775",
   "metadata": {},
   "source": [
    "You can find three short texts below. For each word in the texts, calculate the tf-idf value. Make sure to make each step of your calculation clear (if you repeat steps, you only need to write them down once). You may use a calculator or write some code to get to the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366db3e-5485-46c0-87e0-479026d887fa",
   "metadata": {},
   "source": [
    "### Text 1\n",
    "\n",
    "There are three trees growing in the garden. <br>\n",
    "The trees are very big. <br>\n",
    "They cast big shadows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291fddb-7b62-4b78-9794-56dc9f7fbb51",
   "metadata": {},
   "source": [
    "### Text 2\n",
    "\n",
    "Three kids play in the garden. <br>\n",
    "The shadows of the kids grow big in the evening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61809d20-c294-42d0-b5af-78999fcfa513",
   "metadata": {},
   "source": [
    "### Text 3\n",
    "The people are growing vegetables in the gardens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7b638-68a9-4d18-911c-36885ca2f4cb",
   "metadata": {},
   "source": [
    "Example calculation for 'growing' in Text 3: <br>\n",
    "$C_w = 1$<br>\n",
    "$C_t = 8$<br>\n",
    "$N = 3$<br>\n",
    "$|\\{doc \\in D:w \\in doc\\}| = 2$<br>\n",
    "<br>\n",
    "=> ${\\sf tfidf}({\\sf 'growing'}_{t3}) = \\frac{1}{8}*\\log(\\frac{3}{2}) = 0.05$<br>\n",
    "<br>\n",
    "TF-IDFs:<br>\n",
    "\n",
    "Text 1:<br>\n",
    "there: 0.06<br>\n",
    "are: 0.05<br>\n",
    "three: 0.02<br>\n",
    "trees: 0.13<br>\n",
    "growing: 0.02<br>\n",
    "in: 0.0<br>\n",
    "the: 0.0<br>\n",
    "garden: 0.02<br>\n",
    "very: 0.06<br>\n",
    "big: 0.05<br>\n",
    "they: 0.06<br>\n",
    "cast: 0.06<br>\n",
    "shadows: 0.02<br>\n",
    "\n",
    "Text 2:<br>\n",
    "three: 0.03<br>\n",
    "kids: 0.14<br>\n",
    "play: 0.07<br>\n",
    "in: 0.0<br>\n",
    "the: 0.0<br>\n",
    "garden: 0.03<br>\n",
    "shadows: 0.03<br>\n",
    "of: 0.07<br>\n",
    "grow: 0.07<br>\n",
    "big: 0.03<br>\n",
    "evening: 0.07<br>\n",
    "\n",
    "Text 3:<br>\n",
    "the: 0.0<br>\n",
    "people: 0.14<br>\n",
    "are: 0.05<br>\n",
    "growing: 0.05<br>\n",
    "vegetables: 0.14<br>\n",
    "in: 0.0<br>\n",
    "gardens: 0.14<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb9d82-a254-4bea-933e-52e22faebd86",
   "metadata": {},
   "source": [
    "### Questions\n",
    "Some of the words could have a tf-idf value of 0. Why is this the case? What does that mean conceptually?\n",
    "\n",
    "Did you spot any problems with this application of tf-idf? (Hint: Look at what kinds of linguistic properties are captured or ignored)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3358f-b9b2-4201-a05a-c48881b340fd",
   "metadata": {},
   "source": [
    "If a word appears in all of the documents, it will have a tf-idf value of 0, because $log(\\frac{3}{3}) = 0$. This means that a word that appears in every document of a collection is not considered to be important regarding any text when using the tf-idf metric. <br>\n",
    "The tf-idf measure cannot distinguish between homographs (words spelled the same, but with different meanings) such as 'growing' in text 1 and three. However, it does make a difference between different forms of a word, even if that is not desirable. For example, it treats 'garden' and 'gardens' as separate words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf62148-d31c-4a5f-b6e9-9def57ba10cd",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "An application for the tf-idf values could be to determine the document which is most similar to a given document in a collection. This can be useful, for example, for automatic reading recommendations. <br>\n",
    "For this task, one could use a bag-of-words representation of a document. A bag-of words approach assumes that a document is characterised by the words it contains. <br>\n",
    "Thus, we are going to represent each text from Task 1 as a vector and compare them to each other using the cosine similarity:\n",
    "\n",
    "$\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\|*\\|B\\|}$\n",
    "\n",
    "\n",
    "where $A$ and $B$ are vectors representing the texts to be compared. The higher $\\cos(\\theta)$ is, the smaller the angle between the verctors becomes, which can be interpreted as the two texts being more similar.\n",
    "\n",
    "The vector-representation for each text is a vector with dimension N, where N equals the number of words in the entire vocabulary containing all words that occur in any of the documents in the collection. Each cell of a text-vector corresponds to the tf-idf value of a word in the combined vocabulary with respect to the text the vector should represent.<br>\n",
    "Important: The entries in each vector have to correspond to the same words in all vectors, so, for example, if the first entry of the vector for text 1 contains the tf-idf value for the word \"the\", the first entry of all other text-vectors must also refer to \"the\". <br>\n",
    "\n",
    "Calculate the cosine similarity of text 1 and text 2, and text 1 and text 3. Determine which of the texts 2 and 3 is closer to text 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f4cb0-76d0-46f5-a6c8-9e1dab8b7af3",
   "metadata": {},
   "source": [
    "Vector text 1: [0.06, 0.05, 0.13, 0, 0.06, 0.02, 0.06, 0, 0, 0, 0, 0.06, 0, 0.02, 0.02, 0.0, 0.02, 0.0, 0, 0.05, 0]\n",
    "Vector text 2: [0, 0.03, 0, 0, 0, 0, 0, 0.14, 0, 0.07, 0.07, 0, 0, 0.03, 0.03, 0.0, 0.03, 0.0, 0.07, 0, 0.07]\n",
    "Vector text 3: [0, 0, 0, 0.14, 0, 0.05, 0, 0, 0.14, 0, 0, 0, 0.14, 0, 0, 0.0, 0, 0.0, 0, 0.05, 0]\n",
    "\n",
    "Cosine-similarity between texts 1 and 2: 0.0751<br>\n",
    "Cosine-similarity between texts 1 and 3: 0.0728 <br>\n",
    "Therefore, text 2 is closer to text 1 than text 3. The difference here is rather small, which is mainly due to the texts (and therefore also the total vocabulary) being very short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb32a5-fe38-409d-ab79-df0a760d976b",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "In the lecture, you heard about word embeddings and the Word2Vec algorithm.\n",
    "\n",
    "1. In your own words, describe what word embeddings are.\n",
    "2. Briefly describe how Word2Vec works and what it is used for.\n",
    "3. What are the advantages of representing the words in a text as word embeddings instead of tf-idf values as done in the previous two tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d669ce-b652-462c-bf30-42092374e095",
   "metadata": {},
   "source": [
    "Word embeddings are a vector representation of words. They can have any number of dimensions, but in practice, they usually have around 300 dimensions. The goal of word embeddings is to numerically capture the meaning of a word, by giving it a vector representation that is close to words that are similar in meaning, but far away from words that are very different in the word-vector space. Which is vital for computational language processing, since computers cannot infer meaning from a words written form. <br>\n",
    "<br>\n",
    "Word2Vec is an algorithm that constructs word embeddings for the words in a corpus. For this task, a linear neural network with three layers is used; an input layer with as many input nodes as there are words in the vocabulary of the whole corpus, a hidden layer, for which the number of nodes corresponds to the number of dimensions the embeddings should have, and an output layer, with the same number of nodes as the input layer. To start training the network, first, the size of a context window must be chosen, which defines how far a word can be from another to still be considered to lie in its context. For the next step, one must choose whether to use the skip-gram or CBOW approach. For skip gram, the model is tasked to predict the words in the context window of an input word, while CBOW does the opposite, meaning that the model should predict a word from a given context. The embeddings for each word can then either be extracted from the weights that connect the input and the hidden layer, or from the weights that connect the hidden and the output layer. <br>\n",
    "<br>\n",
    "One advantage of word emeddings is that their representation in a text does not depend on the specific text and collection of documents used in the corpus, and the can therefore be transferred to other NLP applications easily. Additionally, they allow for a much more precise representation of a word's meaning, as they can have an arbitrary number of dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
